# a4

## KNN

For our KKN implementation, we have two functions, one to handle the training data and one to generate predictions on the test data. The training function accepts the input training data file and output model file parameters specified when the program is executed. The training for KNN just consists of saving the entire set of training data to the model file for use later on in the testing function. The majority of the work is performed within the testing function. 

The test function accepts model input file and test data input file as parameters. The model file is the one that was generated by the training function. Both the test data and model data are imported into pandas data frames. Pandas was used so it'd be easier to perform data manipulation in the subsequent steps. Next, a subset of the training data is chosen to use instead of the full training set of data. This was done because it allowed the testing to run much faster than using the entire training data set and only resulted in a 0.8% decrease in prediction accuracy for a 14 minute decrease in runtime. The subset that worked the best during our testing was a 5000 row training data set. We tried numerous combinations of k and training data sizes and this combination of k = 30 and n = 5000 provided the best balance of both performance and accuracy. Below is a table of the combinations of k and training set size that were tested with corresponding run times. 

| k                  | n=500  | n=2500 | n=5000 | n=7500 | n=15000 | n=Full |
|--------------------|--------|--------|--------|--------|---------|--------|
| 5                  | 63.63% |        | 67.23% |        |         | 69.25% |
| 10                 | 65.75% |        | 68.82% |        |         | 71.26% |
| 25                 | 68.29% |        | 70.52% |        |         | 71.79% |
| 30                 | 68.93% | 69.78% | 70.63% | 70.41% | 70.63%  | 72.11% |
| 50                 |        |        | 70.31% |        |         |        |
| 100                | 67.97% |        | 70.1%  |        |         | 71.16% |
| Runtime for k (mi) | < 1    | 1      | 2      | 3      | 6       | 15     |

The size of k had no impact on the overall runtime for a training set size since the distance calculations were already performed before it took the k best into consideration later on. The only thing that really impacted overall test runtimes was the size of the training set. The full set obviously provided the highest accuracy overall, but not by much, so we decided to keep the runtime low for a slight hit to accuracy.

After the subset of the training data is assigned to a data frame, two more variables are created to hold just the image pixel values for the training and test data each. Once all the data we needed was imported and in place, we could then start looping through each of the images in the test set. Inside this loop, we first create a distance tracking dataframe to hold the values generated when calulcating the Euclidian distance between the current test image and all training images. Then the data frames that hold the image data needed to be reshaped and converted to numpy arrays in order to perform vectorized distance calculations between them. A distance forumla is then applied to these two vectors to calculate Euclidian distance between the test image and every image in the training data set. These values are then assigned to the distance tracking dataframe created earlier, where we could then forumate a best guess for each training images orientation compared to the test image based on this distance value. Once we had our best guesses, we could then sort the distance tracking data frame in ascending order and select the top k records of the data frame based on the k value we specified at runtime. The mode of the top k predictions are then selected which will then represent our image orientation prediction which then gets saved to a final predictions data frame.

Once all of the test images have had a prediction made for them, we output the predictions to an output.txt file and then check to see how many predictions we got right compared to the true class label. By far the biggest challenge for this assignment was to get distance calculations between each training image and all of the images in the training data set to run quickly. We tried to vectorize the calculation, but couldn't figure out how to generate predictions for all test images versus all training images instead of looping through each test image one at a time. The best solution that provided a balance of runtime and accuracy was use less of the training data and find that sweet spot that ran quickly and still provided good predictions.
